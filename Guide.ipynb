{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18bedf9d",
   "metadata": {},
   "source": [
    "# Using of OpenVINO custom layers\n",
    "\n",
    "This guide, based on [openvino_pytorch_layers](https://github.com/dkurt/openvino_pytorch_layers) repository, shows how to support [TORCH.NN.FUNCTIONAL.GRID_SAMPLE](https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html#torch-nn-functional-grid-sample) layer for Intel OpenVINO.\n",
    "\n",
    "You can find more information about how to create and use OpenVINO Extensions to facilitate mapping of custom operations from framework model representation to OpenVINO representation [here](https://docs.openvino.ai/latest/openvino_docs_Extensibility_UG_Frontend_Extensions.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3527e093",
   "metadata": {},
   "source": [
    "### 0. Install OpenVINO before running this Jupyter Notebook\n",
    "\n",
    "You can use [this tutorial](https://docs.openvino.ai/latest/openvino_docs_install_guides_installing_openvino_from_archive_linux.html#doxid-openvino-docs-install-guides-installing-openvino-from-archive-linux) to install OpenVINO Runtime.\n",
    "\n",
    "Configure OV environment with `source <INSTALL_DIR>/setupvars.sh`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaff8d1",
   "metadata": {},
   "source": [
    "### 1. Clone repository with extensions examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68725f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/dkurt/openvino_pytorch_layers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ec84a",
   "metadata": {},
   "source": [
    "### 2. Get ONNX sample model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944564bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "\n",
    "!pip install numpy torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceeed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll get `model.onnx` file with model, `inp.npy`, `inp1.npy` files with input tensors \n",
    "# and `ref.npy`file with output for onnx model.\n",
    "\n",
    "!python openvino_pytorch_layers/examples/grid_sample/export_model.py\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192616e3",
   "metadata": {},
   "source": [
    "### 3. Build extensions from sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93149ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll get `libuser_cpu_extension.so` file as result.\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir('./openvino_pytorch_layers/user_ie_extensions')\n",
    "!mkdir build\n",
    "os.chdir('./build')\n",
    "\n",
    "!cmake .. && make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1821b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../../..')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182ec34e",
   "metadata": {},
   "source": [
    "### 4. Infer OpenVINO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b89dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openvino.runtime import Core\n",
    "\n",
    "# Load reference values\n",
    "inp = np.load('inp.npy')\n",
    "inp1 = np.load('inp1.npy')\n",
    "ref_res = np.load('ref.npy')\n",
    "\n",
    "# Create Core and register user extension\n",
    "core = Core()\n",
    "core.add_extension('openvino_pytorch_layers/user_ie_extensions/build/libuser_cpu_extension.so')\n",
    "\n",
    "# You can get .xml and .bin OpenVINO model files with\n",
    "# `mo --input_model model.onnx --extension /path/to/libuser_cpu_extension.so`\n",
    "# or load model from .onnx file directly\n",
    "model = core.read_model('model.onnx')\n",
    "compiled_model = core.compile_model(model, 'CPU')\n",
    "\n",
    "results = compiled_model.infer_new_request({'input': inp, 'input1': inp1})\n",
    "predictions = next(iter(results.values()))\n",
    "\n",
    "# compare ONNX and OV models results\n",
    "diff = np.max(np.abs(ref_res-predictions))\n",
    "print('Res diff: ' + str(diff))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ovn-new-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "0da60746f67872c7338087c870d564cfe73639e93c36b3882c166293c15a519c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
